#!/usr/bin/env python3
"""
Quick Walk-Forward Validation Demo
==================================

Fast demonstration of walk-forward validation with three models:
1. Simple Average (CAMS + NOAA)
2. Ridge Regression  
3. Gradient Boosting

Uses minimal dataset for rapid execution while demonstrating the approach.

Author: Generated by Claude Code
Date: 2025-09-10
"""

import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import warnings
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

class QuickWalkForwardDemo:
    """
    Quick demonstration of walk-forward validation approach.
    """
    
    def __init__(self, dataset_path, output_dir):
        self.dataset_path = Path(dataset_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Target pollutants
        self.pollutants = ['pm25', 'pm10', 'no2', 'o3']
        
        logger.info("Initialized QuickWalkForwardDemo")
    
    def load_sample_data(self):
        """Load and sample data for quick demonstration."""
        logger.info("Loading and sampling dataset...")
        
        # Load dataset
        df = pd.read_csv(self.dataset_path)
        df['datetime'] = pd.to_datetime(df['datetime'])
        df = df.sort_values(['city', 'datetime']).reset_index(drop=True)
        
        # Take last 30 days and sample heavily for speed
        max_date = df['datetime'].max()
        recent_start = max_date - pd.DateOffset(days=30)
        recent_data = df[df['datetime'] >= recent_start].copy()
        
        # Sample every 6th hour (4 times per day) for speed
        sampled_data = recent_data.iloc[::6].copy()
        
        logger.info(f"Sampled to {len(sampled_data):,} records from last 30 days")
        logger.info(f"Date range: {sampled_data['datetime'].min()} to {sampled_data['datetime'].max()}")
        
        # Split into train/test (70/30)
        split_idx = int(len(sampled_data) * 0.7)
        train_data = sampled_data.iloc[:split_idx].copy() 
        test_data = sampled_data.iloc[split_idx:].copy()
        
        logger.info(f"Training: {len(train_data)} records")
        logger.info(f"Testing: {len(test_data)} records")
        
        return train_data, test_data
    
    def prepare_features(self, df):
        """Prepare features for ML models."""
        # Basic features that should work reliably
        feature_cols = [
            'hour', 'month', 'day', 'dayofweek', 'dayofyear',
            'temperature', 'humidity', 'wind_speed', 'pressure',
            'boundary_layer_height', 'traffic_intensity'
        ]
        
        # Only use features that exist and are numeric
        available_features = []
        for col in feature_cols:
            if col in df.columns and df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                available_features.append(col)
        
        logger.info(f"Using features: {available_features}")
        return available_features
    
    def run_demo_validation(self):
        """Run quick walk-forward validation demo."""
        logger.info("Starting quick walk-forward validation demo...")
        
        # Load data
        train_data, test_data = self.load_sample_data()
        
        # Prepare features
        feature_cols = self.prepare_features(train_data)
        
        # Results storage
        results = []
        
        # Process each test record
        current_train = train_data.copy()
        
        for idx, row in test_data.iterrows():
            result_row = {
                'datetime': row['datetime'],
                'city': row['city']
            }
            
            # Add actual values and benchmark forecasts
            for pollutant in self.pollutants:
                result_row[f'actual_{pollutant}'] = row[f'actual_{pollutant}']
                result_row[f'cams_{pollutant}'] = row[f'forecast_cams_{pollutant}']
                result_row[f'noaa_{pollutant}'] = row[f'forecast_noaa_gefs_aerosol_{pollutant}']
                
                # Simple average
                simple_avg = (row[f'forecast_cams_{pollutant}'] + row[f'forecast_noaa_gefs_aerosol_{pollutant}']) / 2
                result_row[f'simple_average_{pollutant}'] = simple_avg
                
                # ML models - train on current training data
                X_train = current_train[feature_cols].fillna(0)
                y_train = current_train[f'actual_{pollutant}']
                X_test = np.array([row[col] if col in row else 0 for col in feature_cols]).reshape(1, -1)
                
                # Ridge regression
                ridge = Ridge(alpha=1.0, random_state=42)
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                ridge.fit(X_train_scaled, y_train)
                ridge_pred = max(0, ridge.predict(X_test_scaled)[0])
                result_row[f'ridge_{pollutant}'] = ridge_pred
                
                # Gradient boosting (simplified)
                gb = GradientBoostingRegressor(n_estimators=20, max_depth=3, random_state=42)
                gb.fit(X_train, y_train)
                gb_pred = max(0, gb.predict(X_test)[0])
                result_row[f'gradient_boosting_{pollutant}'] = gb_pred
            
            results.append(result_row)
            
            # Update training data (walk-forward)
            current_train = pd.concat([current_train, pd.DataFrame([row])], ignore_index=True)
            
            # Progress
            if (len(results)) % 20 == 0:
                logger.info(f"Processed {len(results)}/{len(test_data)} records")
        
        # Convert to DataFrame
        results_df = pd.DataFrame(results)
        
        # Save results
        output_file = self.output_dir / 'quick_walk_forward_demo_results.csv'
        results_df.to_csv(output_file, index=False)
        logger.info(f"Results saved to: {output_file}")
        
        return results_df
    
    def evaluate_and_report(self, results_df):
        """Evaluate models and generate report."""
        logger.info("Evaluating models and generating report...")
        
        models = ['cams', 'noaa', 'simple_average', 'ridge', 'gradient_boosting']
        
        # Calculate metrics
        metrics = []
        for model in models:
            for pollutant in self.pollutants:
                actual_col = f'actual_{pollutant}'
                pred_col = f'{model}_{pollutant}'
                
                if pred_col in results_df.columns:
                    actual = results_df[actual_col].values
                    predicted = results_df[pred_col].values
                    
                    mae = mean_absolute_error(actual, predicted)
                    rmse = np.sqrt(mean_squared_error(actual, predicted))
                    r2 = r2_score(actual, predicted)
                    
                    metrics.append({
                        'model': model,
                        'pollutant': pollutant,
                        'mae': mae,
                        'rmse': rmse,
                        'r2': r2,
                        'n_samples': len(actual)
                    })
        
        metrics_df = pd.DataFrame(metrics)
        
        # Save metrics
        metrics_file = self.output_dir / 'quick_walk_forward_demo_metrics.csv'
        metrics_df.to_csv(metrics_file, index=False)
        
        # Generate report
        report_lines = [
            "=" * 70,
            "QUICK WALK-FORWARD VALIDATION DEMO RESULTS",
            "=" * 70,
            f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"Sample size: {len(results_df)} predictions",
            "",
            "## AVERAGE PERFORMANCE BY MODEL (MAE in ug/m³)",
            ""
        ]
        
        # Average performance
        avg_metrics = metrics_df.groupby('model')['mae'].mean().sort_values()
        
        for model, mae in avg_metrics.items():
            r2_avg = metrics_df[metrics_df['model'] == model]['r2'].mean()
            report_lines.append(f"**{model.upper()}**: MAE={mae:.3f}, R²={r2_avg:.3f}")
        
        # Calculate improvements
        cams_mae = avg_metrics['cams']
        noaa_mae = avg_metrics['noaa']
        
        report_lines.extend([
            "",
            "## ENSEMBLE IMPROVEMENTS:",
            ""
        ])
        
        for model in ['simple_average', 'ridge', 'gradient_boosting']:
            if model in avg_metrics.index:
                model_mae = avg_metrics[model]
                cams_imp = ((cams_mae - model_mae) / cams_mae) * 100
                noaa_imp = ((noaa_mae - model_mae) / noaa_mae) * 100
                
                report_lines.append(f"**{model.upper()}**:")
                report_lines.append(f"  • vs CAMS: {cams_imp:+.1f}%")
                report_lines.append(f"  • vs NOAA: {noaa_imp:+.1f}%")
                report_lines.append("")
        
        # Best model
        best_ensemble = avg_metrics[['simple_average', 'ridge', 'gradient_boosting']].idxmin()
        
        report_lines.extend([
            "## CONCLUSIONS:",
            "",
            f"**Best Ensemble Model**: {best_ensemble.upper()}",
            f"**Best MAE**: {avg_metrics[best_ensemble]:.3f} ug/m³",
            "**Method**: Walk-forward validation with progressive learning",
            "**Note**: This is a demonstration with sampled data",
            "",
            "=" * 70
        ])
        
        # Save and display report
        report_file = self.output_dir / 'quick_walk_forward_demo_report.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        logger.info(f"Report saved to: {report_file}")
        print("\n" + "\n".join(report_lines))
        
        return metrics_df


def main():
    """Main execution function."""
    logger.info("Starting Quick Walk-Forward Validation Demo")
    
    dataset_path = "C:/aqf311/Git_repo/stage_3/data/analysis/5year_hourly_comprehensive_dataset.csv"
    output_dir = "C:/aqf311/Git_repo/stage_4/data/analysis"
    
    demo = QuickWalkForwardDemo(dataset_path, output_dir)
    
    try:
        # Run demo
        results_df = demo.run_demo_validation()
        
        # Evaluate and report
        metrics_df = demo.evaluate_and_report(results_df)
        
        logger.info("Quick walk-forward validation demo completed successfully!")
        
    except Exception as e:
        logger.error(f"Error during demo: {str(e)}")
        raise


if __name__ == "__main__":
    main()