#!/usr/bin/env python3
"""
Walk-Forward Validation Implementation
=====================================

This script implements walk-forward validation for air quality forecasting using three models:
1. Simple Average (CAMS + NOAA benchmark)
2. Ridge Regression
3. Gradient Boosting

The script duplicates the main dataset, removes the last year of predictions, and generates
new predictions using a walk-forward approach that closely simulates real deployment conditions.

Author: Generated by Claude Code
Date: 2025-09-10
"""

import logging
import warnings
from datetime import datetime, timedelta
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")


class WalkForwardValidator:
    """
    Walk-forward validation implementation for air quality forecasting.

    This class implements a realistic deployment simulation where:
    - Training uses all historical data up to prediction point
    - Testing predicts next time period using all available features
    - Models are continuously updated with new observations
    """

    def __init__(self, dataset_path, output_dir):
        self.dataset_path = Path(dataset_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Target pollutants
        self.pollutants = ["pm25", "pm10", "no2", "o3"]
        self.actual_cols = [f"actual_{p}" for p in self.pollutants]
        self.cams_cols = [f"forecast_cams_{p}" for p in self.pollutants]
        self.noaa_cols = [f"forecast_noaa_gefs_aerosol_{p}" for p in self.pollutants]

        # Models
        self.models = {}
        self.scalers = {}

        # Results storage
        self.results = {"simple_average": [], "ridge": [], "gradient_boosting": []}

        logger.info(f"Initialized WalkForwardValidator")
        logger.info(f"Dataset: {self.dataset_path}")
        logger.info(f"Output directory: {self.output_dir}")

    def load_and_prepare_data(self):
        """Load dataset and prepare for walk-forward validation."""
        logger.info("Loading comprehensive dataset...")

        # Load the 5-year dataset
        df = pd.read_csv(self.dataset_path)
        df["datetime"] = pd.to_datetime(df["datetime"])
        df = df.sort_values(["city", "datetime"]).reset_index(drop=True)

        logger.info(f"Loaded {len(df):,} records")
        logger.info(f"Date range: {df['datetime'].min()} to {df['datetime'].max()}")

        # Define last year cutoff (last 365 days)
        max_date = df["datetime"].max()
        last_year_start = max_date - pd.DateOffset(days=365)

        logger.info(f"Last year starts from: {last_year_start}")

        # Split data
        train_data = df[df["datetime"] < last_year_start].copy()
        test_data = df[df["datetime"] >= last_year_start].copy()

        logger.info(
            f"Training data: {len(train_data):,} records ({train_data['datetime'].min()} to {train_data['datetime'].max()})"
        )
        logger.info(
            f"Test data: {len(test_data):,} records ({test_data['datetime'].min()} to {test_data['datetime'].max()})"
        )

        # Prepare feature columns (exclude predictions and actuals)
        exclude_cols = (
            ["city", "datetime", "date"]
            + self.actual_cols
            + self.cams_cols
            + self.noaa_cols
        )
        potential_features = [
            col
            for col in df.columns
            if col not in exclude_cols and not col.startswith("forecast_")
        ]

        # Filter out categorical/string columns
        self.feature_cols = []
        for col in potential_features:
            if df[col].dtype in ["int64", "float64", "int32", "float32"]:
                self.feature_cols.append(col)
            elif (
                col == "week_position"
            ):  # Handle known categorical that should be encoded
                continue  # Skip for now, could be encoded later

        logger.info(f"Using {len(self.feature_cols)} numeric features for ML models")
        logger.info(
            f"Excluded categorical features: {set(potential_features) - set(self.feature_cols)}"
        )

        return train_data, test_data

    def initialize_models(self, train_data):
        """Initialize models for each pollutant."""
        logger.info("Initializing models for each pollutant...")

        for pollutant in self.pollutants:
            self.models[pollutant] = {
                "ridge": Ridge(alpha=1.0, random_state=42),
                "gradient_boosting": GradientBoostingRegressor(
                    n_estimators=100,
                    learning_rate=0.1,
                    max_depth=6,
                    random_state=42,
                    subsample=0.8,
                ),
            }
            self.scalers[pollutant] = StandardScaler()

        logger.info("Models initialized successfully")

    def simple_average_prediction(self, row):
        """Generate simple average prediction from CAMS and NOAA forecasts."""
        predictions = {}
        for pollutant in self.pollutants:
            cams_val = row[f"forecast_cams_{pollutant}"]
            noaa_val = row[f"forecast_noaa_gefs_aerosol_{pollutant}"]
            predictions[pollutant] = (cams_val + noaa_val) / 2
        return predictions

    def train_ml_models(self, train_data, pollutant):
        """Train ML models for a specific pollutant."""
        # Prepare training data
        X_train = train_data[self.feature_cols].fillna(0)
        y_train = train_data[f"actual_{pollutant}"]

        # Remove any infinite values
        X_train = X_train.replace([np.inf, -np.inf], 0)

        # Scale features
        X_train_scaled = self.scalers[pollutant].fit_transform(X_train)

        # Train Ridge
        self.models[pollutant]["ridge"].fit(X_train_scaled, y_train)

        # Train Gradient Boosting
        self.models[pollutant]["gradient_boosting"].fit(X_train_scaled, y_train)

    def predict_ml_models(self, row, pollutant):
        """Generate ML model predictions for a specific pollutant."""
        # Prepare features
        X = np.array([row[col] for col in self.feature_cols]).reshape(1, -1)
        X = np.nan_to_num(X, nan=0, posinf=0, neginf=0)

        # Scale features
        X_scaled = self.scalers[pollutant].transform(X)

        # Generate predictions
        ridge_pred = self.models[pollutant]["ridge"].predict(X_scaled)[0]
        gb_pred = self.models[pollutant]["gradient_boosting"].predict(X_scaled)[0]

        return {
            "ridge": max(0, ridge_pred),  # Ensure non-negative
            "gradient_boosting": max(0, gb_pred),
        }

    def run_walk_forward_validation(self):
        """Execute walk-forward validation process."""
        logger.info("Starting walk-forward validation...")

        # Load and prepare data
        train_data, test_data = self.load_and_prepare_data()

        # Initialize models
        self.initialize_models(train_data)

        # Group test data by city for processing
        cities = test_data["city"].unique()

        # Initialize results storage
        all_results = []

        for city in cities:
            logger.info(f"Processing city: {city}")

            city_test_data = (
                test_data[test_data["city"] == city]
                .sort_values("datetime")
                .reset_index(drop=True)
            )
            city_train_data = train_data[train_data["city"] == city].copy()

            # Process each time step
            for idx, row in city_test_data.iterrows():
                current_datetime = row["datetime"]

                # Re-train models periodically (every 24 hours) with updated training data
                if idx % 24 == 0:  # Retrain daily
                    logger.info(f"  Retraining models at {current_datetime}")

                    for pollutant in self.pollutants:
                        self.train_ml_models(city_train_data, pollutant)

                # Generate predictions for all models
                result_row = {
                    "city": city,
                    "datetime": current_datetime,
                    "forecast_made_date": current_datetime.date(),
                    "forecast_lead_hours": 1,  # 1-hour ahead forecast
                }

                # Add actual values
                for pollutant in self.pollutants:
                    result_row[f"actual_{pollutant}"] = row[f"actual_{pollutant}"]
                    result_row[f"forecast_cams_{pollutant}"] = row[
                        f"forecast_cams_{pollutant}"
                    ]
                    result_row[f"forecast_noaa_gefs_aerosol_{pollutant}"] = row[
                        f"forecast_noaa_gefs_aerosol_{pollutant}"
                    ]

                # Simple Average predictions
                simple_avg_preds = self.simple_average_prediction(row)
                for pollutant in self.pollutants:
                    result_row[f"simple_average_{pollutant}"] = simple_avg_preds[
                        pollutant
                    ]

                # ML model predictions
                for pollutant in self.pollutants:
                    ml_preds = self.predict_ml_models(row, pollutant)
                    result_row[f"ridge_{pollutant}"] = ml_preds["ridge"]
                    result_row[f"gradient_boosting_{pollutant}"] = ml_preds[
                        "gradient_boosting"
                    ]

                all_results.append(result_row)

                # Update training data with current observation (sliding window)
                new_train_row = row.to_dict()
                city_train_data = pd.concat(
                    [city_train_data, pd.DataFrame([new_train_row])], ignore_index=True
                )

                # Progress logging
                if (idx + 1) % 100 == 0:
                    logger.info(
                        f"  Processed {idx + 1}/{len(city_test_data)} records for {city}"
                    )

        # Convert results to DataFrame
        results_df = pd.DataFrame(all_results)

        # Save results
        output_file = self.output_dir / "walk_forward_validation_results.csv"
        results_df.to_csv(output_file, index=False)
        logger.info(f"Results saved to: {output_file}")

        return results_df

    def evaluate_models(self, results_df):
        """Evaluate model performance with comprehensive metrics."""
        logger.info("Evaluating model performance...")

        models_to_evaluate = [
            "simple_average",
            "ridge",
            "gradient_boosting",
            "forecast_cams",
            "forecast_noaa_gefs_aerosol",
        ]
        metrics_results = []

        for model in models_to_evaluate:
            for pollutant in self.pollutants:
                actual_col = f"actual_{pollutant}"

                if model in ["forecast_cams", "forecast_noaa_gefs_aerosol"]:
                    pred_col = f"{model}_{pollutant}"
                else:
                    pred_col = f"{model}_{pollutant}"

                if pred_col in results_df.columns:
                    actual = results_df[actual_col].values
                    predicted = results_df[pred_col].values

                    # Calculate metrics
                    mae = mean_absolute_error(actual, predicted)
                    rmse = np.sqrt(mean_squared_error(actual, predicted))
                    r2 = r2_score(actual, predicted)

                    # Calculate additional metrics
                    mape = np.mean(np.abs((actual - predicted) / (actual + 1e-8))) * 100
                    correlation = np.corrcoef(actual, predicted)[0, 1]

                    metrics_results.append(
                        {
                            "model": model,
                            "pollutant": pollutant,
                            "mae": mae,
                            "rmse": rmse,
                            "r2": r2,
                            "mape": mape,
                            "correlation": correlation,
                            "sample_size": len(actual),
                        }
                    )

        metrics_df = pd.DataFrame(metrics_results)

        # Save metrics
        metrics_file = self.output_dir / "walk_forward_validation_metrics.csv"
        metrics_df.to_csv(metrics_file, index=False)
        logger.info(f"Metrics saved to: {metrics_file}")

        return metrics_df

    def generate_comparison_report(self, metrics_df):
        """Generate comparative analysis report."""
        logger.info("Generating comparison report...")

        report_lines = [
            "=" * 80,
            "WALK-FORWARD VALIDATION RESULTS",
            "=" * 80,
            f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## MODEL PERFORMANCE COMPARISON",
            "",
        ]

        # Overall performance summary
        avg_metrics = (
            metrics_df.groupby("model")
            .agg({"mae": "mean", "rmse": "mean", "r2": "mean", "correlation": "mean"})
            .round(4)
        )

        report_lines.extend(
            ["### Average Performance Across All Pollutants (MAE in μg/m³):", ""]
        )

        for model in avg_metrics.index:
            mae = avg_metrics.loc[model, "mae"]
            r2 = avg_metrics.loc[model, "r2"]
            corr = avg_metrics.loc[model, "correlation"]
            report_lines.append(
                f"**{model.upper()}**: MAE={mae:.3f}, R²={r2:.3f}, Correlation={corr:.3f}"
            )

        report_lines.extend(["", "### Performance by Pollutant:", ""])

        # Performance by pollutant
        for pollutant in self.pollutants:
            pollutant_metrics = metrics_df[
                metrics_df["pollutant"] == pollutant
            ].sort_values("mae")
            report_lines.extend([f"**{pollutant.upper()}**:", ""])

            for _, row in pollutant_metrics.iterrows():
                report_lines.append(
                    f"  • {row['model']}: MAE={row['mae']:.3f}, R²={row['r2']:.3f}"
                )

            report_lines.append("")

        # Calculate improvements
        report_lines.extend(["### Model Improvements Over Benchmarks:", ""])

        cams_mae = avg_metrics.loc["forecast_cams", "mae"]
        noaa_mae = avg_metrics.loc["forecast_noaa_gefs_aerosol", "mae"]

        for model in ["simple_average", "ridge", "gradient_boosting"]:
            if model in avg_metrics.index:
                model_mae = avg_metrics.loc[model, "mae"]
                cams_improvement = ((cams_mae - model_mae) / cams_mae) * 100
                noaa_improvement = ((noaa_mae - model_mae) / noaa_mae) * 100

                report_lines.extend(
                    [
                        f"**{model.upper()}**:",
                        f"  • vs CAMS: {cams_improvement:.1f}% improvement",
                        f"  • vs NOAA: {noaa_improvement:.1f}% improvement",
                        "",
                    ]
                )

        # Best model identification
        best_model = avg_metrics["mae"].idxmin()
        best_mae = avg_metrics.loc[best_model, "mae"]

        report_lines.extend(
            [
                "### CONCLUSION:",
                "",
                f"**Best Performing Model**: {best_model.upper()}",
                f"**Best Average MAE**: {best_mae:.3f} μg/m³",
                f"**Validation Period**: Last 365 days of dataset",
                f"**Validation Method**: Walk-forward with all features",
                "",
                "=" * 80,
            ]
        )

        # Save report
        report_file = self.output_dir / "walk_forward_validation_report.txt"
        with open(report_file, "w") as f:
            f.write("\n".join(report_lines))

        logger.info(f"Report saved to: {report_file}")

        # Also print summary
        print("\n" + "\n".join(report_lines))


def main():
    """Main execution function."""
    logger.info("Starting Walk-Forward Validation Implementation")

    # Paths
    dataset_path = "C:/aqf311/Git_repo/stage_3/data/analysis/5year_hourly_comprehensive_dataset.csv"
    output_dir = "C:/aqf311/Git_repo/stage_4/data/analysis"

    # Initialize validator
    validator = WalkForwardValidator(dataset_path, output_dir)

    try:
        # Run validation
        results_df = validator.run_walk_forward_validation()

        # Evaluate models
        metrics_df = validator.evaluate_models(results_df)

        # Generate report
        validator.generate_comparison_report(metrics_df)

        logger.info("Walk-forward validation completed successfully!")

    except Exception as e:
        logger.error(f"Error during validation: {str(e)}")
        raise


if __name__ == "__main__":
    main()
